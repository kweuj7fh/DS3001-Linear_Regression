{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: EDA and Visualization\n",
    "## Krishu Wadhwa - euj7fh\n",
    "## October 02, 2024\n",
    "\n",
    "Do Q1 or Q2 or Q3, and Q4. You might want to refer to your work for the data wrangling assignment.  \n",
    "Questions Chosen: **Q0, Q1 and Q4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 0\n",
    "\n",
    "Please answer the following questions in your own words.\n",
    "\n",
    "**1. What makes a model \"linear\"? \"Linear\" in what?**  \n",
    "A model is considered \"linear\" when it weighs the explanatory variables optimally to predict an outcome variable, forming a linear relationship between inputs and outputs. They are usually simple and have limited parameters.  \n",
    "\n",
    "**2. How do you interpret the coefficient for a dummy/one-hot-encoded variable? (This is a trick question, and the trick involves how you handle the intercept of the model.)**  \n",
    "The coefficient for a dummy/one-hot-encoded variable represents the difference in the expected value of the dependent variable between that category and the baseline category, holding all other variables constant. The baseline category is implicitly represented by the intercept term in the regression model.\n",
    "\n",
    "**3. Can linear regression be used for classification? Explain why, or why not.**  \n",
    "Linear regression is not ideal for classification because it is designed for predicting continuous outcomes, not categorical labels. While you can separate the outcomes into classes using thresholds, this could lead to poor classification performance or unreliable predictions.\n",
    "\n",
    "**4. What are signs that your linear model is over-fitting?**  \n",
    "Over-fitting is when the model is too complex and fits the training data too closely. This leads to poor outcomes/predictions on unseen data. Thus, the first clear sign that a model is overfitting is if the the model has high accuracy with the training data but low accuracy with the test data. Another sign could be high variance, such as the model fluctuating highly when applied to very similar data.\n",
    "\n",
    "**5. Clearly explain multi-colinearity using the two-stage least squares technique.**  \n",
    "Multicollinearity is when the independent variables in a regression model are highly correlated, and the reason this is an issue is because it is extremely hard to predict the individual effects of the variables (essentially creates a mush). The two-stage least squares technique helps address this issue by first using an instrument variable (related to the problematic variables) to create a new predicted variable. Next, these predicted/estimated values are replaced by the problematic independent variables/predictors in the regression, and this leads to outcomes which are more reliable and accurate.  \n",
    "\n",
    "**6. What are two ways to incorporate nonlinear relationships between your target/response/dependent/outcome variable `y` and your features/control/response/independent variables `x`?**  \n",
    "The first way you could do this would be through feature transformations where we can apply non-linear transformations to existing features. For example, x could be transformed to x^2 or x^3 or even log(x). After transforming these variables, they are then inputted back into the linear regression model, allowing the model to now demonstrate more complex relationships between x and y. Another way to do this is by creating an interaction term. Here, two explanatory variables (such as x_1 and x_2) are taken and then multiplied together to get a new independent/explanatory variable. For example z = x_1 * x_2 could be something like mileage x age. This helps the model to analyze the combined effect of these independent variables on the outcome.\n",
    "\n",
    "**7. What is the interpretation of the intercept? A slope coefficient for a variable? The coefficient for a dummy/one-hot-encoded variable?**  \n",
    "The intercept is the expected value of the outcome variable y when all independent variables (x's) are equal to zero. It is essentially the base value of y (when no independent variables are influencing the outcome). The slope coefficient is the average change of the outcome variable y for a one-unit increase in the independent variable x (given everything else is constant). This shows the strength, as well as the direction (if it is positive or negative) of the linear relationship between x and y. Finally, the coefficient for a dummy/one-hot-encoded variable represents the difference in the expected value of the dependent variable between that category and the baseline/reference category, holding all other variables constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "<br />\n",
    "\n",
    "Load `./data/Q1_clean.csv`. The data include\n",
    "\n",
    "`Price` - per night  \n",
    "`Review Scores Rating` - The average rating for the property  \n",
    "`Neighbourhood` - The bourough of NYC. Note the space, or rename the variable.  \n",
    "`Property Type` - The kind of dwelling  \n",
    "`Room Type` - The kind of space being rented  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Compute the average prices and scores by `Neighbourhood`; which bourough is the most expensive on average? Create a kernel density plot of price and log price, grouping by `Neighbourhood`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Regress price on `Neighbourhood` by creating the appropriate dummy/one-hot-encoded variables, without an intercept in the linear model and using all the data. Compare the coefficients in the regression to the table from part 1. What pattern do you see? What are the coefficients in a regression of a continuous variable on one categorical variable?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Repeat part 2, but leave an intercept in the linear model. How do you have to handle the creation of the dummies differently? What is the intercept? Interpret the coefficients. How can I get the coefficients in part 2 from these new coefficients?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Split the sample 80/20 into a training and a test set. Run a regression of `Price` on `Review Scores Rating` and `Neighbourhood` . What is the `R^2` and `RMSE` on the test set? What is the coefficient on `Review Scores Rating`? What is the most expensive kind of property you can rent?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Split the sample 80/20 into a training and a test set. Run a regression of `Price` on `Review Scores Rating` and `Neighbourhood` and `Property Type`. What is the `R^2` and `RMSE` on the test set? What is the coefficient on `Review Scores Rating`? What is the most expensive kind of property you can rent?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. What does the coefficient on `Review Scores Rating` mean if it changes from part 4 to 5? Hint: Think about how multilple linear regression works.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "<br />\n",
    "\n",
    "This is a math question to review the derivation of the OLS estimator (but only if you are into that kind of thing!). We are going to do it slightly differently from what we did in class, though. We will use a linear predictor and minimize the Sum of Squared Errors, just as in class. But, we are going to de-mean $X$ first, creating another variable $z_i = x_i - \\bar{x}$ where\n",
    "$$\n",
    "\\bar{x} = \\dfrac{1}{N} \\sum_{i=1}^N x_i,\n",
    "$$\n",
    "so the model is $\\hat{y}_i = a + b z_i$ and the `SSE` is\n",
    "$$\n",
    "\\text{SSE}(a,b) = \\sum_{i=1}^N (y_i - a - bz_i)^2.\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Take partial derivatives of the `SSE` with respect to $a$ and $b$. You should get**\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} -2(y_i - a - b z_i) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} -2(y_i - a - b z_i)z_i = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the SSE:\n",
    "\n",
    "$$\n",
    "\\text{SSE}(a, b) = \\sum_{i=1}^{N} (y_i - a - b z_i)^2,\n",
    "$$\n",
    "\n",
    "First we differentiate the SSE with respect to a. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{SSE}}{\\partial a} = \\sum_{i=1}^{N} 2(y_i - a - b z_i)(-1).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{SSE}}{\\partial a} = -2 \\sum_{i=1}^{N} (y_i - a - b z_i).\n",
    "$$\n",
    "\n",
    "$$\n",
    "-2 \\sum_{i=1}^{N} (y_i - a - b z_i) = 0. \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (y_i - a - b z_i) = 0.\n",
    "$$\n",
    "\n",
    "Now we differentiate the SSE with respect to b. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{SSE}}{\\partial b} = \\sum_{i=1}^{N} 2(y_i - a - b z_i)(-z_i).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{SSE}}{\\partial b} = -2 \\sum_{i=1}^{N} (y_i - a - b z_i)z_i.\n",
    "$$\n",
    "\n",
    "$$\n",
    "-2 \\sum_{i=1}^{N} (y_i - a - b z_i)z_i = 0.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (y_i - a - b z_i)z_i = 0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Solve for the solutions to the above equations. Big hint: $\\bar{z} = 0$, since we subtracted the mean of $x$ from $x$ to get $z$. You should get**\n",
    "\n",
    "\\begin{alignat*}{3}\n",
    "a^* &=& \\bar{y} \\\\\n",
    "b^* &=& \\dfrac{\\sum_{i=1}^N(y_i - \\bar{y})z_i}{\\sum_{i=1}^N z_i^2}.\n",
    "\\end{alignat*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can solve for $a^{*}$. \n",
    "\n",
    "$$\n",
    "-2 \\sum_{i=1}^{N} (y_i - a - b z_i) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (y_i - a - b z_i) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (y_i) - aN - b \\sum_{i=1}^{N} (z_i)  = 0\n",
    "$$\n",
    "\n",
    "Since $\\bar{z} = 0$, we have:\n",
    "\n",
    "$$\n",
    "b \\sum_{i=1}^{N} (z_i)  = 0\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\sum_{i=1}^{N} (y_i) - aN  = 0\n",
    "$$\n",
    "\n",
    "If we set the equation to a:\n",
    "$$\n",
    "a = \\frac{\\sum_{i=1}^{N} (y_i)}{N}\n",
    "$$\n",
    "\n",
    "Finally, we get:\n",
    "$$\n",
    "a^{*} = \\bar{y}\n",
    "$$\n",
    "\n",
    "Now,  we can solve for $b^{*}$.\n",
    "\n",
    "$$\n",
    "-2 \\sum_{i=1}^{N} (y_i - a - b z_i)z_i = 0.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (y_i - a - b z_i)z_i = 0.\n",
    "$$\n",
    "\n",
    "Now we substitute $a^{*} = \\bar{y}$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (y_i - \\bar{y} - b z_i)z_i = 0.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (y_i - \\bar{y})(z_i) - b \\sum_{i=1}^{N} (z_i)^2 = 0.\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sum_{i=1}^{N} (y_i - \\bar{y})(z_i) = b \\sum_{i=1}^{N} (z_i)^2\n",
    "$$\n",
    "\n",
    "If we set the equation to b we get:\n",
    "$$\n",
    "b^{*} = \\frac{\\sum_{i=1}^{N} (y_i - \\bar{y})(z_i)}{\\sum_{i=1}^{N} (z_i)^2}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Substitute $z_i = x_i - \\bar{x}$ back into the above equations. You should get**\n",
    "  \n",
    "\\begin{alignat*}{3}\n",
    "a^* &=& \\bar{y} \\\\\n",
    "b^* &=& \\dfrac{\\sum_{i=1}^N(y_i - \\bar{y})(x_i-\\bar{x})}{\\sum_{i=1}^N (x_i-\\bar{x})^2},\n",
    "\\end{alignat*}\n",
    "\n",
    "**which can be written in terms of sample covariance and sample variance as:**\n",
    "\n",
    "\\begin{alignat*}{3}\n",
    "a^* &=& \\bar{y} \\\\\n",
    "b^* &=& \\dfrac{\\text{cov}(x,y)}{\\text{var}(x)}.\n",
    "\\end{alignat*}\n",
    "\n",
    "**This is typically the preferred way of expressing the OLS coefficients.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the last part of the question, we have:\n",
    "$$\n",
    "a^{*} = \\bar{y}  \\\\\n",
    "\\\\  \n",
    "b^{*} = \\frac{\\sum_{i=1}^{N} (y_i - \\bar{y})(z_i)}{\\sum_{i=1}^{N} (z_i)^2}\n",
    "$$\n",
    "\n",
    "Now, if we substitute $z_i = x_i - \\bar{x}$ back into the above equations, we get:\n",
    "\n",
    "$$\n",
    "b^{*} = \\frac{\\sum_{i=1}^{N} (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^{N} (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "Here, the numerator $\\sum_{i=1}^{N} (y_i - \\bar{y})(x_i - \\bar{x})$ represents the sample covariance between $x$ and $y$. Therefore:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (y_i - \\bar{y})(x_i - \\bar{x}) = \\text{cov}(x, y)\n",
    "$$\n",
    "\n",
    "Similarly, the denominator $\\sum_{i=1}^{N} (x_i - \\bar{x})^2$ represents the sample variance of $x$. Therefore:\n",
    "$$\n",
    "\\sum_{i=1}^{N} (x_i - \\bar{x})^2 = \\text{var}(x)\n",
    "$$\n",
    "\n",
    "Substituting both the numerator and denominator back into $b^{*}$, we get:\n",
    "\n",
    "$$\n",
    "a^{*} = \\bar{y} \\\\\n",
    "\n",
    "b^{*} = \\frac{\\text{cov}(x, y)}{\\text{var}(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. When will $b^*$ be large or small, depending on the relationship between $x$ and $y$ and the amount of \"noise\"/variance in $x$? What does $a^*$ represent?**\n",
    "\n",
    "The value of $b^*$ depends on the relationship between the covariance of $x$ and $y$ and the variance of $x$ due to the previous equation from part 3: $b^{*} = \\frac{\\text{cov}(x, y)}{\\text{var}(x)}$.\n",
    "Therefore, $b^{*}$ would be large when there is a strong linear relationship between $x$ and $y$. This is because this would lead to a large covariance $\\text{cov}(x, y)$. $b^{*}$ would also be large in the case when $\\text{var}(x)$ is small. This would be the case when $x$ does not vary highly. On the contrary, $b^{*}$ would be small when there is a weak linear relationship between $x$ and $y$. This is because this would lead to a small covariance $\\text{cov}(x, y)$. Moreover, $b^{*}$ would also be small when $\\text{var}(x)$ is large. This means that $x$ has a lot of variability/\"noise\". $a^{*}$ is the intercept and it represents the expected value of the outcome/response variable $y$. This is due to the fact that $a^{*} = \\bar{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Suppose you have measurement error in $x$ which artificially inflates its variance (e.g. bad data cleaning). What happens to the $b^*$ coefficient? How will affect your ability to predict? (This phenomenon is called **attenuation**.)**\n",
    "\n",
    "A measurement error in $x$ would inflate its variance, causing the slope coefficient $b^*$ to shrink due to attenuation bias. This results in predictions that are biased toward zero, underestimating the true relationship between $x$ and $y$. As such, the model's estimates will be more conservative and less sensitive to changes in $x$, reducing the accuracy and reliability of the model's predictions. This is why cleaning data is an important step in data analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
